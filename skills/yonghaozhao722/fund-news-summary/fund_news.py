#!/usr/bin/env python3
"""
åŸºé‡‘æ–°é—»æ‘˜è¦ç”Ÿæˆå™¨ - å¤šå¸‚åœºç‰ˆæœ¬
æ”¯æŒï¼šçº³æ–¯è¾¾å…‹100ã€æ ‡æ™®500ã€æ¬§æ´²è‚¡å¸‚ã€æ—¥ç»225ã€é»„é‡‘/è´µé‡‘å±
ä½¿ç”¨ QVeris API å’Œ web_search è·å–æ–°é—»ï¼Œç”Ÿæˆä¸­æ–‡æ‘˜è¦æŠ¥å‘Š
"""

import asyncio
import json
import os
import sys
import time
import subprocess
from datetime import datetime
from typing import Dict, List, Optional, Any
import logging

# æ·»åŠ  qveris skill åˆ°è·¯å¾„
QVERIS_PATH = os.path.join(os.path.dirname(__file__), "../qveris")
sys.path.insert(0, QVERIS_PATH)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¸‚åœºé…ç½®
MARKETS = {
    "us": {
        "name": "ç¾è‚¡å¸‚åœº (çº³æ–¯è¾¾å…‹/æ ‡æ™®500)",
        "emoji": "ğŸ‡ºğŸ‡¸",
        "symbols": ["QQQ", "SPY", "AAPL", "MSFT", "NVDA", "GOOGL", "META", "AMZN"],
        "keywords": ["NASDAQ", "S&P 500", "tech", "AI", "èŠ¯ç‰‡"],
        "tool": "finnhub.news.retrieve.v1",
        "tool_params": {"category": "general"}
    },
    "europe": {
        "name": "æ¬§æ´²è‚¡å¸‚",
        "emoji": "ğŸ‡ªğŸ‡º",
        "symbols": ["ASML", "SAP", "MC.PA", "AIR.PA", "NESN.SW"],  # LVMHç”¨MC.PAä»£æ›¿
        "keywords": ["Europe", "ECB", "æ¬§è‚¡", "ASML", "SAP", "LVMH"],
        "tool": "yahoo_finance.finance_news.v1",
        "tool_params": {"max_results": 5}
    },
    "japan": {
        "name": "æ—¥æœ¬è‚¡å¸‚ (æ—¥ç»225)",
        "emoji": "ğŸ‡¯ğŸ‡µ",
        "symbols": ["TM", "SONY", "NTDOY", "HMC", "6758.T"],  # 6758.Tæ˜¯ç´¢å°¼ä¸œäº¬ä»£ç 
        "keywords": ["Nikkei", "Japan", "æ—¥ç»", "ä¸°ç”°", "ç´¢å°¼", "ä»»å¤©å ‚"],
        "tool": "yahoo_finance.finance_news.v1",
        "tool_params": {"max_results": 5}
    },
    "gold": {
        "name": "é»„é‡‘å¸‚åœº",
        "emoji": "ğŸ¥‡",
        "symbols": ["GC=F", "GLD", "XAUUSD"],
        "keywords": ["gold", "XAU", "COMEX", "è´µé‡‘å±", "ç°è´§é»„é‡‘"],
        "tool": "finnhub.news.retrieve.v1",
        "tool_params": {"category": "general"}  # goldæ–°é—»åœ¨generalä¸­
    },
    "polymarket": {
        "name": "Polymarket / é¢„æµ‹å¸‚åœº",
        "emoji": "ğŸ”®",
        "keywords": ["Polymarket", "prediction market", "é¢„æµ‹å¸‚åœº", "å»ä¸­å¿ƒåŒ–é¢„æµ‹"],
        "tool": "web_search"
    }
}


def safe_parse_json_array(json_str: str) -> List[Dict]:
    """
    å®‰å…¨è§£æå¯èƒ½è¢«æˆªæ–­çš„JSONæ•°ç»„
    å¤„ç† truncated_content è¢«æˆªæ–­çš„æƒ…å†µ
    """
    if not json_str or not json_str.strip():
        return []
    
    json_str = json_str.strip()
    
    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        pass
    
    # å¦‚æœè¢«æˆªæ–­ï¼Œå°è¯•ä¿®å¤
    # æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
    if json_str.startswith('['):
        # æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
        objects = []
        current_pos = 1  # è·³è¿‡å¼€å¤´çš„ [
        
        while current_pos < len(json_str):
            # è·³è¿‡ç©ºç™½å’Œé€—å·
            while current_pos < len(json_str) and json_str[current_pos] in ' \t\n\r,':
                current_pos += 1
            
            if current_pos >= len(json_str) or json_str[current_pos] == ']':
                break
            
            # å°è¯•æ‰¾åˆ°ä¸‹ä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
            if json_str[current_pos] == '{':
                brace_count = 1
                in_string = False
                escape_next = False
                obj_start = current_pos
                current_pos += 1
                
                while current_pos < len(json_str) and brace_count > 0:
                    char = json_str[current_pos]
                    
                    if escape_next:
                        escape_next = False
                        current_pos += 1
                        continue
                    
                    if char == '\\':
                        escape_next = True
                        current_pos += 1
                        continue
                    
                    if char == '"' and not escape_next:
                        in_string = not in_string
                        current_pos += 1
                        continue
                    
                    if not in_string:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                    
                    current_pos += 1
                
                if brace_count == 0:
                    # æ‰¾åˆ°äº†ä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
                    try:
                        obj = json.loads(json_str[obj_start:current_pos])
                        objects.append(obj)
                    except:
                        pass
            else:
                current_pos += 1
        
        return objects
    
    return []


class QVerisToolRunner:
    """
    é€šè¿‡å‘½ä»¤è¡Œè°ƒç”¨ QVeris å·¥å…·
    """
    
    QVERIS_DIR = "/root/clawd/skills/qveris"
    
    async def search_tools(self, query: str, limit: int = 5) -> tuple:
        """
        æœç´¢å¯ç”¨å·¥å…·ï¼Œè¿”å› (search_id, tools)
        """
        try:
            cmd = [
                "python3", "scripts/qveris_tool.py", "search", query,
                "--limit", str(limit)
            ]
            result = subprocess.run(
                cmd, cwd=self.QVERIS_DIR,
                capture_output=True, text=True, timeout=30
            )
            
            if result.returncode != 0:
                logger.error(f"æœç´¢å·¥å…·å¤±è´¥: {result.stderr}")
                return None, []
            
            # è§£æè¾“å‡º
            output = result.stdout
            search_id = None
            tools = []
            
            for line in output.split('\n'):
                if line.startswith("Search ID:"):
                    search_id = line.split(":")[1].strip()
                elif line.startswith("[") and "]" in line:
                    # è§£æå·¥å…·ä¿¡æ¯
                    tool_id = None
                    tool_name = ""
                    
            logger.info(f"æœç´¢åˆ°å·¥å…·ï¼Œsearch_id: {search_id}")
            return search_id, tools
            
        except Exception as e:
            logger.error(f"æœç´¢å·¥å…·å¼‚å¸¸: {e}")
            return None, []
    
    async def execute_tool(self, tool_id: str, search_id: str, params: Dict) -> Dict:
        """
        æ‰§è¡ŒæŒ‡å®šå·¥å…·
        """
        try:
            cmd = [
                "python3", "scripts/qveris_tool.py", "execute", tool_id,
                "--search-id", search_id,
                "--params", json.dumps(params)
            ]
            
            result = subprocess.run(
                cmd, cwd=self.QVERIS_DIR,
                capture_output=True, text=True, timeout=60
            )
            
            if result.returncode != 0:
                logger.error(f"æ‰§è¡Œå·¥å…·å¤±è´¥: {result.stderr}")
                return {"success": False, "error": result.stderr}
            
            # è§£æè¾“å‡ºä¸­çš„ JSON
            output = result.stdout
            logger.debug(f"å·¥å…·è¾“å‡º: {output[:500]}...")
            
            # æ‰¾åˆ° JSON å¼€å§‹çš„ä½ç½® (åœ¨ "Result:" ä¹‹å)
            if "Result:" in output:
                json_start = output.find("Result:") + len("Result:")
                json_str = output[json_start:].strip()
                
                # å°è¯•æ‰¾åˆ° JSON çš„ç»“æŸä½ç½®ï¼ˆæŸ¥æ‰¾ç¬¬ä¸€ä¸ªé—­åˆæ‹¬å·åçš„æ¢è¡Œï¼‰
                try:
                    data = json.loads(json_str)
                    return {"success": True, "data": data}
                except json.JSONDecodeError as je:
                    # å¯èƒ½æ˜¯éƒ¨åˆ† JSONï¼Œå°è¯•æå–æœ‰æ•ˆéƒ¨åˆ†
                    logger.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–æœ‰æ•ˆéƒ¨åˆ†: {je}")
                    # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡
                    brace_count = 0
                    in_string = False
                    escape_next = False
                    end_pos = 0
                    
                    for i, char in enumerate(json_str):
                        if escape_next:
                            escape_next = False
                            continue
                        if char == '\\':
                            escape_next = True
                            continue
                        if char == '"' and not escape_next:
                            in_string = not in_string
                            continue
                        if not in_string:
                            if char in '{[':
                                brace_count += 1
                            elif char in '}]':
                                brace_count -= 1
                                if brace_count == 0:
                                    end_pos = i + 1
                                    break
                    
                    if end_pos > 0:
                        try:
                            data = json.loads(json_str[:end_pos])
                            return {"success": True, "data": data}
                        except:
                            pass
                    
                    return {"success": True, "raw": output}
            
            return {"success": True, "raw": output}
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œå·¥å…·å¼‚å¸¸: {e}")
            return {"success": False, "error": str(e)}


class MarketNewsFetcher:
    """
    å¤šå¸‚åœºæ–°é—»è·å–å™¨
    """
    
    # å·²çŸ¥å¯ç”¨çš„å·¥å…·IDï¼ˆåŸºäºæœç´¢ç»“æœï¼‰
    FINNHUB_NEWS_TOOL = "finnhub.news.retrieve.v1.51b2567e"
    YAHOO_NEWS_TOOL = "yahoo_finance.finance_news.v1"
    
    def __init__(self):
        self.qveris = QVerisToolRunner()
        self.search_cache = {}  # ç¼“å­˜æœç´¢ç»“æœ
    
    async def get_us_news(self) -> List[Dict]:
        """
        è·å–ç¾è‚¡æ–°é—» (çº³æ–¯è¾¾å…‹/æ ‡æ™®500)
        ä½¿ç”¨ finnhub.news.retrieve.v1
        """
        logger.info("è·å–ç¾è‚¡æ–°é—»...")
        try:
            # æœç´¢å·¥å…·è·å– search_id
            search_id, _ = await self.qveris.search_tools("US stock market news")
            if not search_id:
                logger.warning("æ— æ³•è·å– search_idï¼Œä½¿ç”¨å¤‡ç”¨æœç´¢ID")
                search_id = "0c006ed7-e7cb-4939-93aa-a3fb2cf69d7b"
            
            result = await self.qveris.execute_tool(
                self.FINNHUB_NEWS_TOOL,
                search_id,
                {"category": "general"}
            )
            
            if not result.get("success"):
                logger.error(f"è·å–ç¾è‚¡æ–°é—»å¤±è´¥: {result.get('error')}")
                return []
            
            data = result.get("data", {})
            
            # è§£ææ–°é—»æ•°æ®
            if data.get("status_code") == 200:
                news_list = data.get("data", [])
                if not news_list:
                    # å°è¯•è§£æ truncated_content (å®ƒæ˜¯ä¸€ä¸ªJSONå­—ç¬¦ä¸²)
                    content = data.get("truncated_content", "")
                    if content:
                        news_list = safe_parse_json_array(content)
                        logger.info(f"ä» truncated_content è§£æåˆ° {len(news_list)} æ¡æ–°é—»")
                else:
                    logger.info(f"ä» data å­—æ®µè·å–åˆ° {len(news_list)} æ¡æ–°é—»")
                
                # ç¡®ä¿ news_list æ˜¯åˆ—è¡¨
                if not isinstance(news_list, list):
                    logger.warning(f"news_list ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(news_list)}")
                    return []
                
                # è¿‡æ»¤ä¸ç§‘æŠ€/çº³æŒ‡ç›¸å…³çš„æ–°é—»
                tech_keywords = ["tech", "AI", "artificial", "chip", "semiconductor", 
                                "nvidia", "apple", "microsoft", "google", "meta", "amazon",
                                "åŠå¯¼ä½“", "èŠ¯ç‰‡", "äººå·¥æ™ºèƒ½", "ç§‘æŠ€", "çº³æ–¯è¾¾å…‹", "nasdaq",
                                "cisco", "micron", "memory", "data center"]
                
                filtered_news = []
                for news in news_list[:20]:  # å–å‰20æ¡
                    if not isinstance(news, dict):
                        continue
                    headline = news.get("headline", "").lower()
                    summary = news.get("summary", "").lower()
                    text = headline + " " + summary
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ç§‘æŠ€è‚¡ç›¸å…³
                    is_tech = any(kw.lower() in text for kw in tech_keywords)
                    
                    news_item = {
                        "title": news.get("headline", ""),
                        "summary": news.get("summary", ""),
                        "source": news.get("source", "Unknown"),
                        "datetime": news.get("datetime", 0),
                        "url": news.get("url", ""),
                        "is_tech": is_tech
                    }
                    filtered_news.append(news_item)
                
                logger.info(f"ç¾è‚¡æ–°é—»ç­›é€‰å®Œæˆï¼Œå…± {len(filtered_news)} æ¡")
                
                # ä¼˜å…ˆè¿”å›ç§‘æŠ€è‚¡ç›¸å…³æ–°é—»
                tech_news = [n for n in filtered_news if n["is_tech"]]
                if tech_news:
                    return tech_news[:5]
                return filtered_news[:5]
            else:
                logger.warning(f"APIè¿”å›é200çŠ¶æ€ç : {data.get('status_code')}")
            
            return []
            
        except Exception as e:
            logger.error(f"è·å–ç¾è‚¡æ–°é—»å¼‚å¸¸: {e}")
            return []
    
    async def get_europe_news(self) -> List[Dict]:
        """
        è·å–æ¬§æ´²è‚¡å¸‚æ–°é—»
        ä½¿ç”¨ yahoo_finance è·å– ASML, SAP, LVMH ç­‰è‚¡ç¥¨æ–°é—»
        """
        logger.info("è·å–æ¬§æ´²è‚¡å¸‚æ–°é—»...")
        try:
            # æœç´¢å·¥å…·è·å– search_id
            search_id, _ = await self.qveris.search_tools("yahoo finance news ASML")
            if not search_id:
                search_id = "a0cdc077-ed28-47a4-88b1-b656ed1b41a6"
            
            all_news = []
            symbols = ["ASML", "SAP"]  # ä¸»è¦å…³æ³¨ ASML å’Œ SAP
            
            for symbol in symbols:
                try:
                    result = await self.qveris.execute_tool(
                        self.YAHOO_NEWS_TOOL,
                        search_id,
                        {"symbol": symbol, "max_results": 3}
                    )
                    
                    if result.get("success"):
                        data = result.get("data", {})
                        articles = data.get("data", {}).get("articles", [])
                        
                        for article in articles[:3]:
                            news_item = {
                                "title": article.get("title", ""),
                                "summary": "",  # Yahooæ–°é—»å¯èƒ½æ²¡æœ‰æ‘˜è¦
                                "source": article.get("publisher", "Yahoo Finance"),
                                "datetime": article.get("pubDate", ""),
                                "url": article.get("link", ""),
                                "symbol": symbol,
                                "is_key_stock": symbol in ["ASML", "SAP"]
                            }
                            all_news.append(news_item)
                            
                except Exception as e:
                    logger.warning(f"è·å– {symbol} æ–°é—»å¤±è´¥: {e}")
                    continue
            
            return all_news[:5]
            
        except Exception as e:
            logger.error(f"è·å–æ¬§æ´²æ–°é—»å¼‚å¸¸: {e}")
            return []
    
    async def get_japan_news(self) -> List[Dict]:
        """
        è·å–æ—¥æœ¬è‚¡å¸‚æ–°é—»
        ä½¿ç”¨ yahoo_finance è·å–ä¸°ç”°ã€ç´¢å°¼ã€ä»»å¤©å ‚ç­‰è‚¡ç¥¨æ–°é—»
        """
        logger.info("è·å–æ—¥æœ¬è‚¡å¸‚æ–°é—»...")
        try:
            search_id, _ = await self.qveris.search_tools("yahoo finance news SONY")
            if not search_id:
                search_id = "a0cdc077-ed28-47a4-88b1-b656ed1b41a6"
            
            all_news = []
            symbols = ["SONY", "NTDOY", "TM"]  # ç´¢å°¼ã€ä»»å¤©å ‚ã€ä¸°ç”°
            
            for symbol in symbols:
                try:
                    result = await self.qveris.execute_tool(
                        self.YAHOO_NEWS_TOOL,
                        search_id,
                        {"symbol": symbol, "max_results": 3}
                    )
                    
                    if result.get("success"):
                        data = result.get("data", {})
                        articles = data.get("data", {}).get("articles", [])
                        
                        for article in articles[:2]:
                            news_item = {
                                "title": article.get("title", ""),
                                "summary": "",
                                "source": article.get("publisher", "Yahoo Finance"),
                                "datetime": article.get("pubDate", ""),
                                "url": article.get("link", ""),
                                "symbol": symbol
                            }
                            all_news.append(news_item)
                            
                except Exception as e:
                    logger.warning(f"è·å– {symbol} æ–°é—»å¤±è´¥: {e}")
                    continue
            
            return all_news[:5]
            
        except Exception as e:
            logger.error(f"è·å–æ—¥æœ¬æ–°é—»å¼‚å¸¸: {e}")
            return []
    
    async def get_gold_news(self) -> List[Dict]:
        """
        è·å–é»„é‡‘å¸‚åœºæ–°é—»
        ä» Finnhub general æ–°é—»ä¸­ç­›é€‰é»„é‡‘ç›¸å…³å†…å®¹ï¼Œæˆ–æœç´¢
        """
        logger.info("è·å–é»„é‡‘å¸‚åœºæ–°é—»...")
        try:
            # å°è¯•ä» Finnhub è·å–æ–°é—»å¹¶ç­›é€‰
            search_id, _ = await self.qveris.search_tools("gold commodity news")
            if not search_id:
                search_id = "0c006ed7-e7cb-4939-93aa-a3fb2cf69d7b"
            
            result = await self.qveris.execute_tool(
                self.FINNHUB_NEWS_TOOL,
                search_id,
                {"category": "general"}
            )
            
            gold_news = []
            
            if result.get("success"):
                data = result.get("data", {})
                
                if data.get("status_code") == 200:
                    news_list = data.get("data", [])
                    if not news_list:
                        content = data.get("truncated_content", "")
                        if content:
                            news_list = safe_parse_json_array(content)
                            logger.info(f"é»„é‡‘æ–°é—»: ä» truncated_content è§£æåˆ° {len(news_list)} æ¡æ–°é—»")
                    else:
                        logger.info(f"é»„é‡‘æ–°é—»: ä» data å­—æ®µè·å–åˆ° {len(news_list)} æ¡æ–°é—»")
                    
                    # ç¡®ä¿ news_list æ˜¯åˆ—è¡¨
                    if not isinstance(news_list, list):
                        logger.warning(f"é»„é‡‘æ–°é—» news_list ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(news_list)}")
                        return []
                    
                    # ç­›é€‰é»„é‡‘ç›¸å…³æ–°é—»
                    gold_keywords = ["gold", "xau", "precious", "metal", "bullion", 
                                     "é»„é‡‘", "è´µé‡‘å±", "comex"]
                    
                    for news in news_list:
                        if not isinstance(news, dict):
                            continue
                        headline = news.get("headline", "").lower()
                        summary = news.get("summary", "").lower()
                        text = headline + " " + summary
                        
                        if any(kw.lower() in text for kw in gold_keywords):
                            news_item = {
                                "title": news.get("headline", ""),
                                "summary": news.get("summary", ""),
                                "source": news.get("source", "Unknown"),
                                "datetime": news.get("datetime", 0),
                                "url": news.get("url", "")
                            }
                            gold_news.append(news_item)
                            
                            if len(gold_news) >= 5:
                                break
                    
                    logger.info(f"é»„é‡‘æ–°é—»ç­›é€‰å®Œæˆï¼Œå…± {len(gold_news)} æ¡")
                else:
                    logger.warning(f"é»„é‡‘æ–°é—» APIè¿”å›é200çŠ¶æ€ç : {data.get('status_code')}")
            
            return gold_news[:5]
            
        except Exception as e:
            logger.error(f"è·å–é»„é‡‘æ–°é—»å¼‚å¸¸: {e}")
            return []

    async def get_polymarket_news(self) -> List[Dict]:
        """
        è·å– Polymarket / é¢„æµ‹å¸‚åœºç›¸å…³æ–°é—»
        ä½¿ç”¨ brave_search.web.search.list.v1 å·¥å…·æœç´¢æœ€æ–°æ–°é—»
        """
        logger.info("è·å– Polymarket / é¢„æµ‹å¸‚åœºæ–°é—»...")
        try:
            # æœç´¢ brave å·¥å…·
            search_id, _ = await self.qveris.search_tools("brave web search")
            if not search_id:
                logger.warning("æ— æ³•è·å–æœç´¢IDï¼Œä½¿ç”¨é»˜è®¤ID")
                search_id = "pm-search-default"
            
            pm_news = []
            brave_tool = "brave_search.web.search.list.v1"
            
            # æœç´¢1: Polymarket æœ€æ–°æ–°é—»
            try:
                result = await self.qveris.execute_tool(
                    brave_tool,
                    search_id,
                    {"q": "Polymarket news latest today", "count": 5}
                )
                
                if result.get("success"):
                    data = result.get("data", {})
                    results = self._parse_search_results(data)
                    
                    logger.info(f"Polymarket æ–°é—»æœç´¢åŸå§‹ç»“æœæ•°: {len(results)}")
                    
                    for item in results[:5]:
                        if isinstance(item, dict):
                            title = item.get("title", item.get("name", ""))
                            # è¿‡æ»¤æ‰ä»‹ç»æ€§/éæ–°é—»å†…å®¹
                            if self._is_news_content(title, item.get("url", "")):
                                desc = item.get("snippet", item.get("description", item.get("content", "")))
                                url = item.get("url", item.get("link", ""))
                                source = self._extract_source(item, url)
                                
                                news_item = {
                                    "title": title,
                                    "summary": desc[:150] if len(desc) > 150 else desc,
                                    "source": source,
                                    "url": url,
                                    "category": "news"
                                }
                                pm_news.append(news_item)
                    
                    logger.info(f"Polymarket æ–°é—»ç­›é€‰å®Œæˆï¼Œå…± {len(pm_news)} æ¡")
                else:
                    logger.warning(f"Polymarket æœç´¢æœªæˆåŠŸ: {result.get('error', 'unknown')}")
            except Exception as e:
                logger.warning(f"Polymarket æœç´¢å¤±è´¥: {e}")
            
            # æœç´¢2: é¢„æµ‹å¸‚åœºè¡Œä¸šåŠ¨æ€
            if len(pm_news) < 3:
                try:
                    result = await self.qveris.execute_tool(
                        brave_tool,
                        search_id,
                        {"q": "prediction market crypto betting news 2025 2026", "count": 4}
                    )
                    
                    if result.get("success"):
                        data = result.get("data", {})
                        results = self._parse_search_results(data)
                        
                        for item in results[:4]:
                            if isinstance(item, dict):
                                title = item.get("title", item.get("name", ""))
                                # é¿å…é‡å¤ä¸”è¿‡æ»¤éæ–°é—»å†…å®¹
                                if not any(n["title"] == title for n in pm_news) and self._is_news_content(title, item.get("url", "")):
                                    desc = item.get("snippet", item.get("description", ""))
                                    url = item.get("url", item.get("link", ""))
                                    source = self._extract_source(item, url)
                                    
                                    news_item = {
                                        "title": title,
                                        "summary": desc[:150] if len(desc) > 150 else desc,
                                        "source": source,
                                        "url": url,
                                        "category": "industry"
                                    }
                                    pm_news.append(news_item)
                        
                        logger.info(f"é¢„æµ‹å¸‚åœºè¡Œä¸šæ–°é—»è¡¥å……å®Œæˆï¼Œå…± {len(pm_news)} æ¡")
                except Exception as e:
                    logger.warning(f"é¢„æµ‹å¸‚åœºè¡Œä¸šæ–°é—»æœç´¢å¤±è´¥: {e}")
            
            # æœç´¢3: é¢„æµ‹å¸‚åœºç‰¹å®šäº‹ä»¶æ–°é—»
            if len(pm_news) < 3:
                try:
                    result = await self.qveris.execute_tool(
                        brave_tool,
                        search_id,
                        {"q": "Polymarket election crypto price prediction results", "count": 3}
                    )
                    
                    if result.get("success"):
                        data = result.get("data", {})
                        results = self._parse_search_results(data)
                        
                        for item in results[:3]:
                            if isinstance(item, dict):
                                title = item.get("title", item.get("name", ""))
                                if not any(n["title"] == title for n in pm_news) and self._is_news_content(title, item.get("url", "")):
                                    desc = item.get("snippet", item.get("description", ""))
                                    url = item.get("url", item.get("link", ""))
                                    source = self._extract_source(item, url)
                                    
                                    news_item = {
                                        "title": title,
                                        "summary": desc[:150] if len(desc) > 150 else desc,
                                        "source": source,
                                        "url": url,
                                        "category": "events"
                                    }
                                    pm_news.append(news_item)
                        
                        logger.info(f"é¢„æµ‹å¸‚åœºäº‹ä»¶æ–°é—»è¡¥å……å®Œæˆï¼Œå…± {len(pm_news)} æ¡")
                except Exception as e:
                    logger.warning(f"é¢„æµ‹å¸‚åœºäº‹ä»¶æ–°é—»æœç´¢å¤±è´¥: {e}")
            
            return pm_news[:6]
            
        except Exception as e:
            logger.error(f"è·å– Polymarket æ–°é—»å¼‚å¸¸: {e}")
            return []
    
    def _parse_search_results(self, data: dict) -> list:
        """è§£ææœç´¢ç»“æœ"""
        results = []
        if isinstance(data, dict):
            if "data" in data and isinstance(data["data"], dict):
                results = data["data"].get("results", data["data"].get("web", {}).get("results", []))
            elif "results" in data:
                results = data["results"]
            elif "web" in data and isinstance(data["web"], dict):
                results = data["web"].get("results", [])
            elif "organic" in data:
                results = data["organic"]
        return results
    
    def _is_news_content(self, title: str, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ–°é—»å†…å®¹è€Œéä»‹ç»é¡µé¢"""
        if not title:
            return False
        
        title_lower = title.lower()
        url_lower = url.lower() if url else ""
        
        # æ’é™¤ä»‹ç»æ€§/ä¸»é¡µå†…å®¹
        exclude_patterns = [
            "| the world's largest prediction market",
            "| polymarket",
            "- wikipedia",
            "home | ",
            "official site",
            "about us",
            "what is",
            "how to",
            "guide to",
            "tutorial",
            "wiki",
            "definition",
            "meaning of"
        ]
        
        for pattern in exclude_patterns:
            if pattern in title_lower:
                return False
        
        # æ’é™¤å®˜ç½‘ä¸»é¡µå’Œç»´åŸºç™¾ç§‘
        if "polymarket.com" in url_lower and ("blog" not in url_lower and "news" not in url_lower):
            return False
        if "wikipedia.org" in url_lower:
            return False
        
        # åŒ…å«æ–°é—»ç‰¹å¾
        news_indicators = [
            "news", "report", "says", "announces", "launches", "raises", 
            "acquires", "partners", "expands", "grows", "surges", "drops",
            "regulators", "sec", "cftc", "lawsuit", "legal", "ban", "approval",
            "trading", "volume", "market", "prediction", "betting", "election",
            "crypto", "blockchain", "defi", "web3"
        ]
        
        return any(ind in title_lower for ind in news_indicators)
    
    def _extract_source(self, item: dict, url: str) -> str:
        """æå–æ–°é—»æ¥æº"""
        source = item.get("source", item.get("domain", ""))
        if not source and url:
            try:
                from urllib.parse import urlparse
                parsed = urlparse(url)
                source = parsed.netloc.replace("www.", "")
            except:
                source = "News"
        if not source:
            source = "News"
        return source


class FundNewsGenerator:
    """åŸºé‡‘æ–°é—»ç”Ÿæˆå™¨ - å¤šå¸‚åœºç‰ˆæœ¬"""
    
    def __init__(self):
        self.fetcher = MarketNewsFetcher()
    
    def translate_to_chinese(self, text: str) -> str:
        """
        ç®€å•çš„è‹±æ–‡æ–°é—»æ ‡é¢˜/æ‘˜è¦ç¿»è¯‘é€»è¾‘
        è¿™é‡Œå¯ä»¥æ¥å…¥ç¿»è¯‘APIï¼Œæš‚æ—¶è¿”å›åŸæ–‡å¹¶åšç®€å•å¤„ç†
        """
        if not text:
            return "æš‚æ— æ‘˜è¦"
        
        # å¦‚æœå·²ç»æ˜¯ä¸­æ–‡ï¼Œç›´æ¥è¿”å›
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            return text
        
        # è¿”å›åŸæ–‡ï¼ˆæˆªæ–­ï¼‰
        return text[:120] + "..." if len(text) > 120 else text
    
    async def generate_market_report(self) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„å¸‚åœºæ–°é—»æ‘˜è¦æŠ¥å‘Š
        """
        logger.info("å¼€å§‹æ”¶é›†å„å¸‚åœºæ–°é—»...")
        start_time = time.monotonic()
        
        # å¹¶è¡Œè·å–æ‰€æœ‰å¸‚åœºæ–°é—»
        results = await asyncio.gather(
            self.fetcher.get_us_news(),
            self.fetcher.get_europe_news(),
            self.fetcher.get_japan_news(),
            self.fetcher.get_gold_news(),
            self.fetcher.get_polymarket_news(),
            return_exceptions=True
        )
        
        us_news = results[0] if not isinstance(results[0], Exception) else []
        europe_news = results[1] if not isinstance(results[1], Exception) else []
        japan_news = results[2] if not isinstance(results[2], Exception) else []
        gold_news = results[3] if not isinstance(results[3], Exception) else []
        polymarket_news = results[4] if not isinstance(results[4], Exception) else []
        
        elapsed = time.monotonic() - start_time
        logger.info(f"æ–°é—»è·å–å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f} ç§’")
        
        # æ„å»ºæŠ¥å‘Š
        today_str = datetime.now().strftime("%Y-%m-%d")
        report_lines = [
            f"ğŸ“Š **åŸºé‡‘æ–°é—»æ‘˜è¦** ({today_str})",
            ""
        ]
        
        # ğŸ‡ºğŸ‡¸ ç¾è‚¡å¸‚åœº
        report_lines.extend([
            "ğŸ‡ºğŸ‡¸ **ç¾è‚¡å¸‚åœº (çº³æ–¯è¾¾å…‹/æ ‡æ™®500)**",
            ""
        ])
        if us_news:
            for news in us_news[:4]:
                title = news.get("title", "æ— æ ‡é¢˜")
                summary = self.translate_to_chinese(news.get("summary", ""))
                source = news.get("source", "æœªçŸ¥")
                report_lines.append(f"â€¢ **{title}**")
                report_lines.append(f"  â”” {summary} (*{source}*)")
                report_lines.append("")
        else:
            report_lines.append("â€¢ æš‚æ— æœ€æ–°ç¾è‚¡æ–°é—»")
            report_lines.append("")
        
        # ğŸ‡ªğŸ‡º æ¬§æ´²è‚¡å¸‚
        report_lines.extend([
            "ğŸ‡ªğŸ‡º **æ¬§æ´²è‚¡å¸‚**",
            ""
        ])
        if europe_news:
            for news in europe_news[:4]:
                title = news.get("title", "æ— æ ‡é¢˜")
                source = news.get("source", "æœªçŸ¥")
                symbol = news.get("symbol", "")
                symbol_tag = f" [{symbol}]" if symbol else ""
                report_lines.append(f"â€¢ **{title}**{symbol_tag}")
                report_lines.append(f"  â”” (*{source}*)")
                report_lines.append("")
        else:
            report_lines.append("â€¢ æš‚æ— æœ€æ–°æ¬§æ´²è‚¡å¸‚æ–°é—»")
            report_lines.append("")
        
        # ğŸ‡¯ğŸ‡µ æ—¥æœ¬è‚¡å¸‚
        report_lines.extend([
            "ğŸ‡¯ğŸ‡µ **æ—¥æœ¬è‚¡å¸‚ (æ—¥ç»225)**",
            ""
        ])
        if japan_news:
            for news in japan_news[:4]:
                title = news.get("title", "æ— æ ‡é¢˜")
                source = news.get("source", "æœªçŸ¥")
                symbol = news.get("symbol", "")
                symbol_tag = f" [{symbol}]" if symbol else ""
                report_lines.append(f"â€¢ **{title}**{symbol_tag}")
                report_lines.append(f"  â”” (*{source}*)")
                report_lines.append("")
        else:
            report_lines.append("â€¢ æš‚æ— æœ€æ–°æ—¥æœ¬è‚¡å¸‚æ–°é—»")
            report_lines.append("")
        
        # ğŸ¥‡ é»„é‡‘å¸‚åœº
        report_lines.extend([
            "ğŸ¥‡ **é»„é‡‘å¸‚åœº**",
            ""
        ])
        if gold_news:
            for news in gold_news[:4]:
                title = news.get("title", "æ— æ ‡é¢˜")
                summary = self.translate_to_chinese(news.get("summary", ""))
                source = news.get("source", "æœªçŸ¥")
                report_lines.append(f"â€¢ **{title}**")
                report_lines.append(f"  â”” {summary} (*{source}*)")
                report_lines.append("")
        else:
            report_lines.append("â€¢ æš‚æ— æœ€æ–°é»„é‡‘æ–°é—»")
            report_lines.append("")
        
        # ğŸ”® Polymarket / é¢„æµ‹å¸‚åœº
        report_lines.extend([
            "ğŸ”® **Polymarket / é¢„æµ‹å¸‚åœº**",
            ""
        ])
        if polymarket_news:
            for news in polymarket_news[:4]:
                title = news.get("title", "æ— æ ‡é¢˜")
                summary = news.get("summary", "")[:100]
                if summary and len(news.get("summary", "")) > 100:
                    summary += "..."
                source = news.get("source", "æœªçŸ¥")
                category = news.get("category", "")
                cat_tag = f" [{category}]" if category else ""
                report_lines.append(f"â€¢ **{title}**{cat_tag}")
                if summary:
                    report_lines.append(f"  â”” {summary} (*{source}*)")
                else:
                    report_lines.append(f"  â”” (*{source}*)")
                report_lines.append("")
        else:
            report_lines.append("â€¢ æš‚æ— æœ€æ–°é¢„æµ‹å¸‚åœºæ–°é—»")
            report_lines.append("")
        
        # ğŸ¯ åŸºé‡‘ç›¸å…³è¦ç‚¹
        report_lines.extend([
            "ğŸ¯ **åŸºé‡‘ç›¸å…³è¦ç‚¹**",
            ""
        ])
        
        # æ ¹æ®è·å–åˆ°çš„æ–°é—»ç”Ÿæˆè¦ç‚¹
        if us_news:
            report_lines.append("â€¢ çº³æ–¯è¾¾å…‹100: AIæ¿å—æŒç»­å—å…³æ³¨ï¼Œç§‘æŠ€è‚¡æ³¢åŠ¨éœ€ç•™æ„")
        else:
            report_lines.append("â€¢ çº³æ–¯è¾¾å…‹100: å…³æ³¨AIå’ŒèŠ¯ç‰‡æ¿å—åŠ¨æ€")
            
        report_lines.append("â€¢ æ ‡æ™®500: å…³æ³¨ç¾è”å‚¨æ”¿ç­–å’Œä¼ä¸šè´¢æŠ¥å­£è¡¨ç°")
        
        if europe_news:
            report_lines.append("â€¢ æ¬§æ´²åŠ¨åŠ›: ASMLã€SAPç­‰é¾™å¤´ä¼ä¸šè®¢å•åŠä¸šç»©è¡¨ç°")
        else:
            report_lines.append("â€¢ æ¬§æ´²åŠ¨åŠ›: å…³æ³¨æ¬§æ´²å¤®è¡Œè´§å¸æ”¿ç­–åŠåˆ¶é€ ä¸šæ•°æ®")
            
        if japan_news:
            report_lines.append("â€¢ æ—¥æœ¬ç²¾é€‰: ä¸°ç”°ã€ç´¢å°¼ã€ä»»å¤©å ‚ç­‰æ ¸å¿ƒæŒä»“åŠ¨æ€")
        else:
            report_lines.append("â€¢ æ—¥æœ¬ç²¾é€‰: å…³æ³¨æ—¥å…ƒæ±‡ç‡å’Œæ—¥æœ¬å¤®è¡Œæ”¿ç­–åŠ¨å‘")
            
        if gold_news:
            report_lines.append("â€¢ é»„é‡‘ETF: é¿é™©éœ€æ±‚æ¨åŠ¨é‡‘ä»·æ³¢åŠ¨")
        else:
            report_lines.append("â€¢ é»„é‡‘ETF: å…³æ³¨åœ°ç¼˜æ”¿æ²»é£é™©å’Œç¾å…ƒæŒ‡æ•°èµ°åŠ¿")
        
        # Polymarket / é¢„æµ‹å¸‚åœºè¦ç‚¹
        if polymarket_news:
            report_lines.append("â€¢ é¢„æµ‹å¸‚åœº: Polymarket å¹³å°æ´»è·ƒåº¦åŠçƒ­é—¨äº‹ä»¶å€¼å¾—è·Ÿè¸ª")
            report_lines.append("â€¢ åŠ å¯†é¢„æµ‹: å»ä¸­å¿ƒåŒ–é¢„æµ‹å¸‚åœºä¸åŠ å¯†è´§å¸è”åŠ¨æ€§å¢å¼º")
        else:
            report_lines.append("â€¢ é¢„æµ‹å¸‚åœº: å…³æ³¨ Polymarket ç­‰å¹³å°çƒ­é—¨é¢„æµ‹äº‹ä»¶")
            report_lines.append("â€¢ è‡ªåŠ¨åŒ–äº¤æ˜“: é¢„æµ‹å¸‚åœºé‡åŒ–ç­–ç•¥åŠè·Ÿå•æœºä¼š")
        
        report_lines.append("")
        
        # âš ï¸ é£é™©æç¤º
        report_lines.extend([
            "âš ï¸ **é£é™©æç¤º**",
            ""
        ])
        report_lines.append("â€¢ ç¾è‚¡å¸‚åœºæ³¢åŠ¨å¯èƒ½å—ç¾è”å‚¨æ”¿ç­–åŠç§‘æŠ€è‚¡è¡¨ç°å½±å“")
        report_lines.append("â€¢ æ¬§æ´²è‚¡å¸‚å—åœ°ç¼˜æ”¿æ²»åŠæ¬§æ´²å¤®è¡Œè´§å¸æ”¿ç­–å½±å“")
        report_lines.append("â€¢ æ—¥æœ¬è‚¡å¸‚å—æ—¥å…ƒæ±‡ç‡æ³¢åŠ¨åŠæ—¥æœ¬å¤®è¡Œæ”¿ç­–å½±å“")
        report_lines.append("â€¢ é»„é‡‘ä»·æ ¼å—åœ°ç¼˜æ”¿æ²»é£é™©åŠç¾å…ƒæŒ‡æ•°èµ°åŠ¿å½±å“")
        report_lines.append("â€¢ é¢„æµ‹å¸‚åœºå­˜åœ¨æµåŠ¨æ€§é£é™©åŠç›‘ç®¡ä¸ç¡®å®šæ€§")
        report_lines.append("â€¢ è‡ªåŠ¨åŒ–äº¤æ˜“ç­–ç•¥éœ€ä¸¥æ ¼é£æ§ï¼Œé¿å…è¿‡åº¦æ æ†")
        report_lines.append("â€¢ QDIIåŸºé‡‘å—æ±‡ç‡æ³¢åŠ¨å½±å“ï¼ŒæŠ•èµ„éœ€è°¨æ…")
        report_lines.append("")
        
        # æ•°æ®æ¥æº
        report_lines.append(f"â±ï¸ è·å–è€—æ—¶: {elapsed:.1f}ç§’ | ğŸ“° æ•°æ®æ¥æº: QVeris API / Finnhub / Yahoo Finance")
        
        return "\n".join(report_lines)
    
    async def generate_simple_report(self) -> str:
        """
        ç”Ÿæˆç®€åŒ–ç‰ˆæŠ¥å‘Šï¼ˆå½“APIä¸å¯ç”¨æ—¶ï¼‰
        """
        today_str = datetime.now().strftime("%Y-%m-%d")
        report_lines = [
            f"ğŸ“Š **åŸºé‡‘æ–°é—»æ‘˜è¦** ({today_str})",
            "",
            "ğŸ‡ºğŸ‡¸ **ç¾è‚¡å¸‚åœº (çº³æ–¯è¾¾å…‹/æ ‡æ™®500)**",
            "â€¢ AIæ¿å—æŒç»­å—å…³æ³¨ï¼Œç§‘æŠ€è‚¡æ³¢åŠ¨éœ€ç•™æ„",
            "â€¢ ç¾è”å‚¨æ”¿ç­–é¢„æœŸå½±å“å¸‚åœºæƒ…ç»ª",
            "",
            "ğŸ‡ªğŸ‡º **æ¬§æ´²è‚¡å¸‚**",
            "â€¢ å…³æ³¨ASMLã€SAPç­‰é¾™å¤´ä¼ä¸šåŠ¨æ€",
            "â€¢ æ¬§æ´²å¤®è¡Œè´§å¸æ”¿ç­–ç»´æŒå®½æ¾é¢„æœŸ",
            "",
            "ğŸ‡¯ğŸ‡µ **æ—¥æœ¬è‚¡å¸‚ (æ—¥ç»225)**",
            "â€¢ å…³æ³¨æ—¥å…ƒæ±‡ç‡å’Œæ—¥æœ¬å¤®è¡Œæ”¿ç­–åŠ¨å‘",
            "â€¢ å‡ºå£ä¼ä¸šå—ç›Šäºæ±‡ç‡å˜åŒ–",
            "",
            "ğŸ¥‡ **é»„é‡‘å¸‚åœº**",
            "â€¢ åœ°ç¼˜æ”¿æ²»é£é™©æ¨å‡é¿é™©éœ€æ±‚",
            "â€¢ å…³æ³¨ç¾å…ƒæŒ‡æ•°èµ°åŠ¿",
            "",
            "ğŸ”® **Polymarket / é¢„æµ‹å¸‚åœº**",
            "â€¢ å…³æ³¨å¹³å°çƒ­é—¨é¢„æµ‹äº‹ä»¶åŠäº¤æ˜“é‡å˜åŒ–",
            "â€¢ å»ä¸­å¿ƒåŒ–é¢„æµ‹å¸‚åœºä¸åŠ å¯†è´§å¸è”åŠ¨",
            "",
            "âš ï¸ **é£é™©æç¤º**",
            "â€¢ é¢„æµ‹å¸‚åœºå­˜åœ¨ç›‘ç®¡ä¸ç¡®å®šæ€§å’ŒæµåŠ¨æ€§é£é™©",
            "â€¢ è‡ªåŠ¨åŒ–äº¤æ˜“éœ€ä¸¥æ ¼é£æ§",
            "â€¢ QDIIåŸºé‡‘å—æ±‡ç‡æ³¢åŠ¨å½±å“ï¼ŒæŠ•èµ„éœ€è°¨æ…",
            "",
            "ğŸ“Š æ•°æ®æ¥æº: å¤‡ç”¨æ•°æ®"
        ]
        return "\n".join(report_lines)


async def main():
    """ä¸»å‡½æ•°"""
    generator = FundNewsGenerator()
    
    try:
        # å°è¯•ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        report = await generator.generate_market_report()
        
        print(report)
        
        # ä¿å­˜åˆ° Obsidian vault (ä¸­æ–‡ç‰ˆæœ¬)
        today_str = datetime.now().strftime("%Y-%m-%d")
        obsidian_dir = "/root/clawd/obsidian-vault/reports/fund"
        os.makedirs(obsidian_dir, exist_ok=True)
        output_path = os.path.join(obsidian_dir, f"{today_str}.md")
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report)
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ° {output_path}")
        
        # è‡ªåŠ¨åŒæ­¥åˆ° GitHub (å…ˆæäº¤æœ¬åœ°æ›´æ”¹ï¼Œå† pullï¼Œå† push)
        try:
            import subprocess
            os.chdir("/root/clawd/obsidian-vault")
            
            # å…ˆæäº¤æœ¬åœ°æ›´æ”¹
            logger.info("æ­£åœ¨æäº¤æœ¬åœ°æ›´æ”¹...")
            subprocess.run(["git", "add", "-A"], check=False)
            commit_result = subprocess.run(
                ["git", "commit", "-m", f"Update fund report {today_str}"],
                capture_output=True, text=True
            )
            if commit_result.returncode == 0:
                logger.info("æœ¬åœ°æ›´æ”¹å·²æäº¤")
            
            # è·å–è¿œç¨‹æ›´æ–° (ä½¿ç”¨ merge ç­–ç•¥)
            logger.info("æ­£åœ¨åŒæ­¥ GitHub ä»“åº“...")
            pull_result = subprocess.run(
                ["git", "pull", "origin", "master", "--no-rebase"],
                capture_output=True, text=True
            )
            if pull_result.returncode == 0:
                logger.info("æˆåŠŸæ‹‰å–è¿œç¨‹æ›´æ–°")
            else:
                logger.warning(f"æ‹‰å–è¿œç¨‹æ›´æ–°å¤±è´¥: {pull_result.stderr}")
            
            # æ¨é€
            push_result = subprocess.run(
                ["git", "push", "origin", "master"],
                capture_output=True, text=True
            )
            
            if push_result.returncode == 0:
                logger.info("âœ… å·²è‡ªåŠ¨åŒæ­¥åˆ° GitHub")
            else:
                logger.warning(f"GitHub æ¨é€å¤±è´¥: {push_result.stderr}")
                
        except Exception as e:
            logger.warning(f"GitHub åŒæ­¥å¤±è´¥: {e}")
        
        return report
        
    except Exception as e:
        logger.exception("ç”ŸæˆæŠ¥å‘Šå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆ")
        # é™çº§åˆ°ç®€åŒ–æŠ¥å‘Š
        try:
            report = await generator.generate_simple_report()
            print(report)
            return report
        except Exception as e2:
            logger.error(f"ç®€åŒ–æŠ¥å‘Šä¹Ÿå¤±è´¥: {e2}")
            raise


if __name__ == "__main__":
    asyncio.run(main())
